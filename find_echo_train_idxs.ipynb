{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this file we simply are interested to find out which k-space column lines are acquired in which echo trains. \n",
    "And how many echo trains are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import twixtools\n",
    "from pathlib import Path\n",
    "import ismrmrd\n",
    "from assets.operations_kspace import get_first_acquisition, echo_train_count, echo_train_length, get_num_averages\n",
    "from assets.util import setup_logger\n",
    "\n",
    "\n",
    "# All patient IDs to consider for Uncertainty Quantification\n",
    "pat_ids = [\n",
    "    # '0003_ANON5046358',\n",
    "    # '0004_ANON9616598',\n",
    "    # '0005_ANON8290811',\n",
    "    # '0006_ANON2379607',\n",
    "    # '0007_ANON1586301',\n",
    "    # '0008_ANON8890538',\n",
    "    # '0010_ANON7748752',\n",
    "    # '0011_ANON1102778',\n",
    "    # '0012_ANON4982869',\n",
    "    # '0013_ANON7362087',\n",
    "    # '0014_ANON3951049',\n",
    "    # '0015_ANON9844606',\n",
    "    # '0018_ANON9843837',\n",
    "    # '0019_ANON7657657',\n",
    "    # '0020_ANON1562419',\n",
    "    # '0021_ANON4277586',\n",
    "    # '0023_ANON6964611',\n",
    "    # '0024_ANON7992094',\n",
    "    # '0026_ANON3620419',\n",
    "    # '0027_ANON9724912',\n",
    "    # '0028_ANON3394777',\n",
    "    # '0029_ANON7189994',\n",
    "    # '0030_ANON3397001',\n",
    "    # '0031_ANON9141039',\n",
    "    # '0032_ANON7649583',\n",
    "    # '0033_ANON9728185',\n",
    "    # '0035_ANON3474225',\n",
    "    # '0036_ANON0282755',\n",
    "    # '0037_ANON0369080',\n",
    "    # '0039_ANON0604912',\n",
    "    # '0042_ANON9423619',\n",
    "    # '0043_ANON7041133',\n",
    "    # '0044_ANON8232550',\n",
    "    # '0045_ANON2563804',\n",
    "    # '0047_ANON3613611',\n",
    "    # '0048_ANON6365688',\n",
    "    # '0049_ANON9783006',\n",
    "    # '0051_ANON1327674',\n",
    "    # '0052_ANON9710044',\n",
    "    # '0053_ANON5517301',\n",
    "    # '0055_ANON3357872',\n",
    "    # '0056_ANON2124757',\n",
    "    # '0057_ANON1070291',\n",
    "    # '0058_ANON9719981',\n",
    "    # '0059_ANON7955208',\n",
    "    # '0061_ANON7642254',\n",
    "    # '0062_ANON0319974',\n",
    "    # '0063_ANON9972960',\n",
    "    # '0064_ANON0282398',\n",
    "    # '0067_ANON0913099',\n",
    "    # '0068_ANON7978458',\n",
    "    # '0069_ANON9840567',\n",
    "    # '0070_ANON5223499',\n",
    "    # '0071_ANON9806291',\n",
    "    # '0073_ANON5954143',\n",
    "    # '0075_ANON5895496',\n",
    "    # '0076_ANON3983890',\n",
    "    # '0077_ANON8634437',\n",
    "    # '0078_ANON6883869',\n",
    "    # '0079_ANON8828023',\n",
    "    # '0080_ANON4499321',\n",
    "    # '0081_ANON9763928',\n",
    "    # '0082_ANON6073234',\n",
    "    # '0083_ANON9898497',\n",
    "    # '0084_ANON6141178',\n",
    "    # '0085_ANON4535412',\n",
    "    # '0086_ANON8511628',\n",
    "    # '0087_ANON9534873',\n",
    "    # '0088_ANON9892116',\n",
    "    # '0089_ANON9786899',\n",
    "    # '0090_ANON0891692',\n",
    "    # '0092_ANON9941969',\n",
    "    # '0093_ANON9728761',\n",
    "    # '0094_ANON8024204',\n",
    "    # '0095_ANON4189062',\n",
    "    # '0097_ANON5642073',\n",
    "    # '0103_ANON8583296',\n",
    "    # '0104_ANON7748630',\n",
    "    # '0105_ANON9883201',\n",
    "    # '0107_ANON4035085',\n",
    "    # '0108_ANON0424679',\n",
    "    # '0109_ANON9816976',\n",
    "    # '0110_ANON8266491',\n",
    "    # '0111_ANON9310466',\n",
    "    # '0112_ANON3210850',\n",
    "    # '0113_ANON9665113',\n",
    "    # '0115_ANON0400743',\n",
    "    # '0116_ANON9223478',\n",
    "    # '0118_ANON7141024',\n",
    "    # '0119_ANON3865800',\n",
    "    # '0120_ANON7275574',\n",
    "    # '0121_ANON9629161',\n",
    "    # '0123_ANON7265874',\n",
    "    # '0124_ANON8610762',\n",
    "    # '0125_ANON0272089',\n",
    "    # '0126_ANON4747182',\n",
    "    # '0127_ANON8023509',\n",
    "    # '0128_ANON8627051',\n",
    "    # '0129_ANON5344332',\n",
    "    # '0135_ANON9879440',\n",
    "    # '0136_ANON8096961',\n",
    "    # '0137_ANON8035619',\n",
    "    # '0138_ANON1747790',\n",
    "    # '0139_ANON2666319',\n",
    "    # '0140_ANON0899488',\n",
    "    # '0141_ANON8018038',\n",
    "    # '0142_ANON7090827',\n",
    "    # '0143_ANON9752849',\n",
    "    # '0144_ANON2255419',\n",
    "    # '0145_ANON0335209',\n",
    "    # '0146_ANON7414571',\n",
    "    # '0148_ANON9604223',\n",
    "    # '0149_ANON4712664',\n",
    "    # '0150_ANON5824292',\n",
    "    # '0152_ANON2411221',\n",
    "    # '0153_ANON5958718',\n",
    "    # '0155_ANON7828652',\n",
    "    # '0157_ANON9873056',\n",
    "    # '0159_ANON9720717',\n",
    "    '0160_ANON3504149',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore ISMRMRD for a moment to understand basic functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for kspace files in E:\\01_data\\01_prostate_raw_kspace_umcg_p1\\0003_patient_umcg_done\\kspaces\n",
      "Files found: 0\n"
     ]
    }
   ],
   "source": [
    "roots = {\n",
    "    'raw_kspace': Path(f'E:/01_data/01_prostate_raw_kspace_umcg_p1'),\n",
    "    '0160_mrd_fpath': Path(f'F:/01_data/02_processed_pst_ksp_umcg_p2/data/pat_data/0160_ANON3504149/mrds/meas_MID00150_FID60477_t2_tse_tra_p2_384-out_2.mrd')\n",
    "}\n",
    "\n",
    "pat_id = '0003'\n",
    "kspace_dir = roots['raw_kspace'] / f'{pat_id}_patient_umcg_done/kspaces'\n",
    "\n",
    "print(f\"Looking for kspace files in {kspace_dir}\")\n",
    "\n",
    "# find all files that look like: meas_MID00401_FID373323_t2_tse_traobl_p2_384.dat. The ID is not important, but the rest is.\n",
    "print(f\"Files found: {len(list(kspace_dir.glob('meas_MID*_FID*_t2_tse_traobl_p2_384.dat')))}\")\n",
    "\n",
    "t2_tse_files = list(kspace_dir.glob('meas_MID*_FID*_t2_tse_traobl_p2_384.dat'))\n",
    "for dat_file in t2_tse_files:\n",
    "    print(f\"Processing {dat_file}\")\n",
    "    file_size = dat_file.stat().st_size  # size in bytes\n",
    "    print(f\"File size: {file_size / (1024**2):.2f} MB\")\n",
    "    if file_size < 1 * 1024**3:\n",
    "        print(f\"Skipping {dat_file}, size {file_size} bytes is smaller than 1GB.\")\n",
    "        continue\n",
    "    print(f\"Processing {dat_file} (size: {file_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_large_kspace_files(pat_id: str, kspace_dat_path: Path, pattern='meas_MID*_FID*_t2_tse_traobl_p2_384.dat', min_size=1 * 1024**3):\n",
    "    \"\"\"\n",
    "    Find and return kspace files for a given patient that match the glob pattern and are larger than min_size.\n",
    "\n",
    "    Parameters:\n",
    "        pat_id (str): Patient identifier.\n",
    "        kspace_dat_path (Path): Path to the directory containing kspace files.\n",
    "        pattern (str): Glob pattern to search for.\n",
    "        min_size (int): Minimum file size in bytes (default is 1GB).\n",
    "\n",
    "    Returns:\n",
    "        list: List of file paths (Path objects) that satisfy the criteria.\n",
    "    \"\"\"\n",
    "    kspace_dir = kspace_dat_path / f'{pat_id}_patient_umcg_done/kspaces'\n",
    "    valid_files = []\n",
    "    for dat_file in kspace_dir.glob(pattern):\n",
    "        file_size = dat_file.stat().st_size\n",
    "        if file_size >= min_size:\n",
    "            valid_files.append(dat_file)\n",
    "            print(f\"Found valid file: {dat_file}, size: {file_size / (1024**2):.2f} MB\")\n",
    "        else:\n",
    "            print(f\"Skipping {dat_file}, size {file_size} bytes is smaller than 1GB.\")\n",
    "    \n",
    "    print(f\"Found {len(valid_files)} valid kspace files for patient {pat_id} in {kspace_dir}\")\n",
    "    return valid_files\n",
    "\n",
    "\n",
    "def import_kspace(mdb_list):\n",
    "    \"\"\"\n",
    "    Read image data from a list of mdbs and sort into a 5D k-space array.\n",
    "    \n",
    "    The resulting array has shape:\n",
    "        [n_avg, n_part, n_line, n_channel, n_column]\n",
    "    where:\n",
    "      - n_avg: number of averages (cAve),\n",
    "      - n_part: number of partitions (cPar),\n",
    "      - n_line: number of phase-encode lines (cLin),\n",
    "      - n_channel, n_column: dimensions from the data shape.\n",
    "      \n",
    "    This function sums the data from each acquisition into the proper bin,\n",
    "    which takes care of averaging if multiple acquisitions per index exist.\n",
    "    \n",
    "    Parameters:\n",
    "        mdb_list (list): List of mdb objects.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 5D numpy array containing the sorted k-space data.\n",
    "    \"\"\"\n",
    "    image_mdbs = [mdb for mdb in mdb_list if mdb.is_image_scan()]\n",
    "    \n",
    "    n_line = 1 + max(mdb.cLin for mdb in image_mdbs)\n",
    "    n_part = 1 + max(mdb.cPar for mdb in image_mdbs)\n",
    "    n_ave  = 1 + max(mdb.cAve for mdb in image_mdbs)  # average dimension\n",
    "    n_channel, n_column = image_mdbs[0].data.shape\n",
    "\n",
    "    print(f'Found nline to be: {n_line}')\n",
    "    print(f'Found npart to be: {n_part}')\n",
    "    print(f'Found nAve to be: {n_ave}')\n",
    "    print(f'Found nchannel to be: {n_channel}')\n",
    "    print(f'Found ncolumn to be: {n_column}')\n",
    "\n",
    "    # Initialize a 5D array: average, partition, line, channel, column\n",
    "    out = np.zeros([n_ave, n_part, n_line, n_channel, n_column], dtype=np.complex64)\n",
    "    \n",
    "    for mdb in image_mdbs:\n",
    "        # The '+=' operator handles cases where multiple acquisitions fall into the same bin.\n",
    "        out[mdb.cAve, mdb.cPar, mdb.cLin] += mdb.data\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_echo_trains_idxs(mdb_list, etl: int) -> dict:\n",
    "    image_mdbs = [mdb for mdb in mdb_list if mdb.is_image_scan()]\n",
    "    if not image_mdbs:\n",
    "        return {}\n",
    "    \n",
    "    # Total lines as given by cLin counters (0-indexed)\n",
    "    n_line_total = 1 + max(mdb.cLin for mdb in image_mdbs)\n",
    "    print(f'Found nline_total to be: {n_line_total}')\n",
    "\n",
    "    num_echo_trains = n_line_total // etl  # assuming it divides evenly\n",
    "    print(f'Number of echo trains: {num_echo_trains}')\n",
    "    \n",
    "    mapping = {}\n",
    "    # Map each acquired line (starting at line=1 up to n_line_total-1)\n",
    "    for line in range(1, n_line_total):\n",
    "        index = line - 1  # convert to 0-index for mapping\n",
    "        echo_train = index // etl\n",
    "        line_within_echo = index % etl\n",
    "        mapping[line] = (echo_train, line_within_echo)\n",
    "    \n",
    "    return {\n",
    "        'total_lines': n_line_total,\n",
    "        'num_echo_trains': num_echo_trains,\n",
    "        'mapping': mapping\n",
    "    }\n",
    "    \n",
    "\n",
    "def calculate_first_average_echo_mapping(mdb_list, etl: int) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate echo train mapping for the first average from a list of mdb objects.\n",
    "    \n",
    "    This function filters the mdb objects for image scans in the first average (cAve == 0),\n",
    "    and then computes a mapping from each echo train index to a list of acquired line indices\n",
    "    (cLin) that belong to that echo train. The echo train index is calculated as:\n",
    "    \n",
    "        echo_train = cLin // etl\n",
    "    \n",
    "    Parameters:\n",
    "        mdb_list (list): List of mdb objects.\n",
    "        etl (int): Echo Train Length.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping each echo train index (int) to a sorted list of acquired\n",
    "              line indices (list of ints) from the first average.\n",
    "    \"\"\"\n",
    "    # Filter for image scans in the first average\n",
    "    # first_avg = [mdb for mdb in mdb_list if mdb.is_image_scan() and mdb.cAve == 0]\n",
    "    first_avg = [mdb for mdb in mdb_list if mdb.is_image_scan()]\n",
    "    \n",
    "    mapping = {}\n",
    "    for mdb in first_avg:\n",
    "        echo_train = mdb.cLin // etl\n",
    "        mapping.setdefault(echo_train, []).append(mdb.cLin)\n",
    "    \n",
    "    # Sort the acquired line indices for each echo train\n",
    "    for key in mapping:\n",
    "        mapping[key].sort()\n",
    "        print(f\"Echo train {key}: {mapping[key]}\")\n",
    "\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def process_patient(pat_id: str, kspace_dat_path: Path):\n",
    "    \"\"\"\n",
    "    Process a single patient by finding valid kspace files and summarizing each using twixtools.\n",
    "\n",
    "    Parameters:\n",
    "        pat_id (str): Patient identifier.\n",
    "        kspace_dat_path (Path): Path to the directory containing kspace files.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing patient: {pat_id}\")\n",
    "    valid_files = find_large_kspace_files(pat_id, kspace_dat_path)\n",
    "    if not valid_files:\n",
    "        print(f\"No valid kspace files found for patient {pat_id}\")\n",
    "        return\n",
    "    for file_path in valid_files:\n",
    "        print(f\"Processing {file_path}\")\n",
    "\n",
    "        twix_obj = twixtools.read_twix(str(file_path), parse_geometry=False, parse_data=True)\n",
    "        print(f\"Number of elements in twix_obj = {len(twix_obj)}\")\n",
    "        for k, v in twix_obj[-1].items():       # we take the last one, which is the image data\n",
    "            print(f\"  {k}: value to big for printing\")\n",
    "\n",
    "        imaging_data = twix_obj[-1]\n",
    "        mapping = calculate_first_average_echo_mapping(imaging_data['mdb'], etl=25)\n",
    "        idxs_echo_trains = extract_echo_trains_idxs(imaging_data['mdb'], etl=25)\n",
    "\n",
    "        print(\"Start of SETS\")\n",
    "        for k, v in mapping.items():\n",
    "            # print(f\"  {k}: {v}\")\n",
    "            print(f\"Set of {k}: {set(v)}, len: {len(set(v))}\")\n",
    "        # kspace_array = import_kspace(imaging_data['mdb'])\n",
    "        # print(\"K-space array shape:\", kspace_array.shape)\n",
    "\n",
    "if False:\n",
    "    # Example usage: loop over multiple patients.\n",
    "    pat_ids = ['0003', '0004', '0005']\n",
    "    pat_ids = ['0003']\n",
    "\n",
    "    for pat in pat_ids:\n",
    "        process_patient(pat, roots['raw_kspace'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we just get it from the conversion loop from .mrd to h5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_echo_train_mapping(fpath_mrd: str) -> Dict[int, List[int]]:\n",
    "    \"\"\"\n",
    "    This function builds a k-space array from a .mrd file.\n",
    "    \n",
    "    Parameters:\n",
    "    - fpath_mrd (str): Path to the .mrd file.\n",
    "    \n",
    "    Returns:\n",
    "    - echo_train_mapping (dict): A mapping of echo train numbers to their corresponding indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"\\tBuilding kspace array from .mrd file\")\n",
    "\n",
    "    dset   = ismrmrd.Dataset(fpath_mrd, create_if_needed=False)\n",
    "    header = ismrmrd.xsd.CreateFromDocument(dset.read_xml_header())\n",
    "    enc    = header.encoding[0]\n",
    "\n",
    "    ncoils     = header.acquisitionSystemInformation.receiverChannels\n",
    "    nslices    = enc.encodingLimits.slice.maximum + 1 if enc.encodingLimits.slice is not None else 1\n",
    "    eNy        = enc.encodedSpace.matrixSize.y\n",
    "    rNx        = enc.reconSpace.matrixSize.x\n",
    "    eTL        = 25 if DEBUG else echo_train_length(dset, verbose=True)                           # echo train length = 25\n",
    "    # eTC        = 13 if DEBUG else echo_train_count(dset, echo_train_len=eTL, verbose=True)       # echo train count = 11\n",
    "    firstacq   = get_first_acquisition(dset)\n",
    "    navgs      = 3 #if DEBUG else get_num_averages(firstacq=firstacq, dset=dset)\n",
    "    total_acqs = dset.number_of_acquisitions()\n",
    "    logger.info(f\"\\t navgs: {navgs}, nslices: {nslices}, ncoils: {ncoils}, rNx: {rNx}, eNy: {eNy}, first_qc: {firstacq}, total_acqs: {total_acqs}\")\n",
    "\n",
    "    # Loop through the rest of the acquisitions and fill the data array with the kspace data\n",
    "    echo_train_mapping = {}\n",
    "    init_col_idx = -1\n",
    "    for acq_idx in range(firstacq, dset.number_of_acquisitions()):\n",
    "        acq          = dset.read_acquisition(acq_idx)\n",
    "        slice_idx    = acq.idx.slice\n",
    "        col_idx      = acq.idx.kspace_encode_step_1\n",
    "        avg_idx      = acq._head.idx.average\n",
    "        \n",
    "        if init_col_idx == -1:\n",
    "            init_col_idx = col_idx\n",
    "        elif init_col_idx == col_idx:\n",
    "            print(f\"Skipping acquisition {acq_idx} because col_idx is the same as init_col_idx\", end=\"\",)\n",
    "            continue\n",
    "\n",
    "        if acq_idx > 1000:\n",
    "            break\n",
    "\n",
    "        logger.info(f\"i: {acq_idx}, avg_idx: {avg_idx}, col_idx: {col_idx}, cSlc: {slice_idx}\")\n",
    "\n",
    "    return echo_train_mapping\n",
    "\n",
    "\n",
    "DEBUG  = False\n",
    "logger = setup_logger(Path('logs'), namepart=\"pat0160\" , use_time=False)\n",
    "\n",
    "echo_mapping = get_echo_train_mapping(roots['0160_mrd_fpath'])\n",
    "for k, v in echo_mapping.items():\n",
    "    print(f\"Set of {k}: {set(v)}, len: {len(set(v))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
